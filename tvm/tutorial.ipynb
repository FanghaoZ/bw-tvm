{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutotial is about one of the case we have done to run an e2e test with the TVM framework using our Brainwave FPGA hardware.\n",
    "\n",
    "It more like a POC to verify that we can use TVM to help us improve our BrianWave development. So please feel free to add more funtions as your like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running it, you need to create a conda env and install required packedge:\n",
    "\n",
    "    conda create -n bw-tvm python=3.8\n",
    "    conda activate bw-tvm\n",
    "    pip install mlc-ai-nightly -f https://mlc.ai/wheels\n",
    "    pip intall -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code we need to run any firmware test in BrainSlice repo to build the target files.\n",
    "Here we need to add PYTHONPATH to where the BrainSlice repo is.  \n",
    "1.Set the BRAINSLICE_PATH to the BrainSlice repo  \n",
    "2.Set SKU_PATH to the SKU fils you want to test.  \n",
    "3.FIRMWARE_PATH to the firmware you want to call from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "BRAINSLICE_PATH = 'D:/BrainSlice-repo/develop/BrainSlice/'\n",
    "BRAINSLICE_TARGET_PATH = BRAINSLICE_PATH + 'target/distrib/retail/x64/app/BrainSlice/'\n",
    "SKU_PATH = BRAINSLICE_PATH + 'src/config/skugen/obj/amd64/BERT-NP/SKU.json'\n",
    "FIRMWARE_LIB_PATH = BRAINSLICE_TARGET_PATH + '/Firmware/content'\n",
    "EMULATOR_PATH = BRAINSLICE_TARGET_PATH + '/DevKit/lib/native/python'\n",
    "FIRMWARE_PATH = FIRMWARE_LIB_PATH + '/BERT'\n",
    "sys.path.append(EMULATOR_PATH)\n",
    "sys.path.append(FIRMWARE_LIB_PATH)\n",
    "import brainslice_client as bs_client\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T\n",
    "import torch\n",
    "import numpy as np\n",
    "from ISA import Mem\n",
    "import Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Create IR model from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "import onnx\n",
    "import numpy as np\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "\n",
    "######################################################################\n",
    "# Generated MLP ONNX model\n",
    "# ---------------------------------------------\n",
    "def export_onnx_model():\n",
    "    # Define a simple MLP in PyTorch\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = nn.functional.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Create an instance of the model\n",
    "    model = MLP(input_size=784, hidden_size=256, output_size=10)\n",
    "\n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.randn(1, 784)\n",
    "\n",
    "    # Export the model to ONNX format\n",
    "    torch.onnx.export(model, dummy_input, \"mlp.onnx\")\n",
    "\n",
    "######################################################################\n",
    "# Unit test for TVM from ONNX\n",
    "# ---------------------------------------------\n",
    "def test_compile_onnx_model():\n",
    "    # Load ONNX model\n",
    "    onnx_model = onnx.load('mlp.onnx')\n",
    "    input_name = 'onnx::Gemm_0'\n",
    "    \n",
    "\n",
    "    # Convert the ONNX model to Relay IR\n",
    "    input_shape = {input_name: (1, 784)}\n",
    "    mod, params = relay.frontend.from_onnx(onnx_model, shape=input_shape)\n",
    "\n",
    "    # Compile the model with relay\n",
    "    target = 'llvm'\n",
    "        \n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "    dev = tvm.device(str(target), 0)\n",
    "    module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "        \n",
    "    # Execute on TVM\n",
    "    dtype = \"float32\"\n",
    "    dummy_input = torch.randn(1, 784)\n",
    "    input_data = np.asarray(dummy_input).astype(dtype)\n",
    "    module.set_input(input_name, input_data)\n",
    "    module.run()\n",
    "    output_shape = (1, 10)\n",
    "    tvm_output = module.get_output(0, tvm.nd.empty(output_shape)).numpy()\n",
    "\n",
    "    print(\"TVM_output: \", tvm_output)\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Golden reference from ONNX runtime\n",
    "    sess = rt.InferenceSession(\"mlp.onnx\", providers=rt.get_available_providers())\n",
    "\n",
    "    input_name0 = sess.get_inputs()[0].name\n",
    "\n",
    "    golden_onnx = sess.run(None, {input_name0: input_data})\n",
    "    print(\"golden_onnx: \", np.array(golden_onnx).flatten())\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Calculate the MSE\n",
    "    loss = np.square(np.subtract(tvm_output, golden_onnx)).mean()\n",
    "\n",
    "    print(\"MSE: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frontend is compiling the ONNX file into Relay IR, while our backend is using Relax IR, how to translate Relay IR into Rleax IR may need further investagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regisiter externl funtions to call FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'bw-tvm' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n bw-tvm ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def create_fpga_session(mode='fp32', sku_dir=\" \", firmware_dir=\" \", firmware_name=\" \", fpga_chip_id=0, ):\n",
    "    _mode = {'fp32': bs_client.Backend.EMULATOR_FLOAT,\n",
    "             'quantized': bs_client.Backend.EMULATOR_QUANTIZED,\n",
    "             'FPGA': bs_client.Backend.FPGA}[mode]\n",
    "    directory_path = Path(firmware_dir)\n",
    "    bs_sess = bs_client.Session(_mode, Path(sku_dir), giano_instance_id=0, fpga_chip_id=fpga_chip_id, fpga_request_timeout=600.0)\n",
    "    bs_sess.start_session()\n",
    "    if mode == 'FPGA':\n",
    "        print(\"FPGA Session started,id: \", fpga_chip_id)\n",
    "        bs_sess.load_firmware(firmware_directory=directory_path,\n",
    "                              firmware_name=firmware_name)\n",
    "    else :\n",
    "        print(\"Emulator Session started,mode:\", mode)\n",
    "        bs_sess.load_emulator_firmware(firmware_directory=directory_path,\n",
    "                                           firmware_name=firmware_name)\n",
    "    global _native_dim\n",
    "    _native_dim = bs_sess.parameters.NATIVE_DIM\n",
    "    print(\"Firmware loaded successfully\")\n",
    "    return bs_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.register_func(\"bs.linear\",override=True)\n",
    "def bs_linear(x: tvm.nd.NDArray,\n",
    "              w: tvm.nd.NDArray,\n",
    "              b: tvm.nd.NDArray,\n",
    "              out: tvm.nd.NDArray):\n",
    "    ############################\n",
    "    #create fpga session\n",
    "    ############################\n",
    "    bs_sess = create_fpga_session(mode='fp32', sku_dir=SKU_PATH, firmware_dir=FIRMWARE_PATH, firmware_name=\"BERT\")\n",
    "\n",
    "    ############################\n",
    "    #init linear\n",
    "    ############################\n",
    "    # checke size\n",
    "    num_vec = x.shape[0]\n",
    "    num_col = x.shape[1]\n",
    "    num_row = w.shape[0]\n",
    "\n",
    "    assert w.shape[1] == num_col\n",
    "    assert b.shape[0] == num_row\n",
    "\n",
    "    x = x.numpy()\n",
    "    w = w.numpy()\n",
    "    b = b.numpy()\n",
    "    z = torch.zeros(num_vec, num_row, dtype=torch.float32)\n",
    "    out_torch = torch.from_dlpack(out)\n",
    "\n",
    "    # set the last element in MVM_IVRF to zero-vector\n",
    "    bs_sess.load_vector(np.zeros(bs_sess.parameters.NATIVE_DIM), Mem.MvmInitialVrf, bs_sess.parameters.MVM_INITIAL_VRF_SIZE - 1)\n",
    "\n",
    "    # SLU-specific matrix loading\n",
    "    if (bs_sess.parameters.ENABLE_FIXED_FUNCTION_SLU):\n",
    "        i_mat = np.identity(bs_sess.parameters.NATIVE_DIM)\n",
    "        bs_sess.load_matrix(i_mat, address = 0, memory = Mem.MatrixRf)\n",
    "\n",
    "    args = Bert.FullyConnectedParams(\n",
    "            rows = num_row,\n",
    "            cols = num_col,\n",
    "            bias = True,\n",
    "            gelu = True,\n",
    "            relu = False,\n",
    "            vecs = num_vec,\n",
    "            x_startaddr = 0,\n",
    "            w_startaddr = 1,\n",
    "            b_startaddr = 0,\n",
    "            imat_addr = 0,\n",
    "            asvrf1_scratchpad = 0,\n",
    "            mfu_scratchpad = 0,\n",
    "            use_dram = True,\n",
    "            weight_dram_addr = 0,\n",
    "            bias_dram_addr = 0,\n",
    "    )\n",
    "    # load bias and weight\n",
    "    bs_sess.load_vector(b, address = args.bias_dram_addr, memory = Mem.Dram)\n",
    "    bs_sess.load_matrix(w, address = args.weight_dram_addr, memory = Mem.Dram)\n",
    "    ############################\n",
    "    #run linear\n",
    "    ############################\n",
    "    bs_res = bs_sess.run(Bert.FullyConnectedLayer(args, inputVector = x))  # execute matrix to vector multiplications\n",
    "    torch.add(torch.asarray(bs_res[\"outputVector\"]), z, out=out_torch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Build relax IR module to call our FPGA.\n",
    "Ideally, this module could be automatically generated. With patten matching and translate, we can turn\n",
    "\n",
    "    @tvm.script.ir_module\n",
    "        class LinearModule:\n",
    "            @R.function\n",
    "            def main(x: R.Tensor((\"v\", \"c\"), \"float32\"),\n",
    "                     w: R.Tensor((\"c\", \"r\"), \"float32\"),\n",
    "                     b: R.Tensor((\"r\", ), \"float32\")\n",
    "                     ) -> R.Tensor((\"v\", \"r\"), \"float32\"):\n",
    "                v, c, r= T.int64(), T.int64(), T.int64()\n",
    "                with R.dataflow():\n",
    "                    z1 = R.matmul(x, w)\n",
    "                    z2 = R.add(z1, b)\n",
    "                    z3 = R.nn.gelu(z2)\n",
    "                    R.output(z3)\n",
    "                return z3\n",
    "\n",
    "to\n",
    "\n",
    "    @tvm.script.ir_module\n",
    "        class LinearModule:\n",
    "            @R.function\n",
    "            def main(x: R.Tensor((\"v\", \"c\"), \"float32\"),\n",
    "                    w: R.Tensor((\"r\", \"c\"), \"float32\"),\n",
    "                    b: R.Tensor((\"r\", ), \"float32\")\n",
    "                    ) -> R.Tensor((\"v\", \"r\"), \"float32\"):\n",
    "                v, c, r= T.int64(), T.int64(), T.int64()\n",
    "                with R.dataflow():\n",
    "                    z = R.call_dps_packed(\"bs.linear\", (x, w, b), R.Tensor((v, r), \"float32\"))\n",
    "                    R.output(lv0)\n",
    "                return z\n",
    "\n",
    "However, we have not solved this problem yet. So here we use our handwritten relax IR module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class ManualLinear:\n",
    "    @R.function\n",
    "    def main(x: R.Tensor((\"v\", \"c\"), \"float32\"),\n",
    "             w: R.Tensor((\"r\", \"c\"), \"float32\"),\n",
    "             b: R.Tensor((\"r\", ), \"float32\")\n",
    "             ) -> R.Tensor((\"v\", \"r\"), \"float32\"):\n",
    "        v, c, r= T.int64(), T.int64(), T.int64()\n",
    "        with R.dataflow():\n",
    "            lv0 = R.call_dps_packed(\"bs.linear\", (x, w, b), R.Tensor((v, r), \"float32\"))\n",
    "            R.output(lv0)\n",
    "        return lv0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Build and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emulator Session started,mode: fp32\n",
      "Firmware loaded successfully\n",
      "tensor([[-1.6996e-01, -1.5929e-05,  8.9435e-01,  1.0604e+00, -3.5851e-03,\n",
      "          0.0000e+00,  9.3218e+00,  0.0000e+00,  4.9787e-01,  3.6981e+00,\n",
      "         -2.2366e-04,  0.0000e+00, -5.4929e-03,  8.8425e+00,  5.9610e+00,\n",
      "         -1.6971e-01]])\n",
      "[[-1.7004395e-01 -5.3644180e-06  8.9746094e-01  1.0615234e+00\n",
      "  -3.1871796e-03  0.0000000e+00  9.3281250e+00  0.0000000e+00\n",
      "   4.9902344e-01  3.6953125e+00 -1.3720989e-04  0.0000000e+00\n",
      "  -5.0773621e-03  8.8437500e+00  5.9609375e+00 -1.6979980e-01]]\n"
     ]
    }
   ],
   "source": [
    "num_vec = 1\n",
    "num_col = 32\n",
    "num_row = 16\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "x = torch.randn((num_vec, num_col), dtype=torch.float32)\n",
    "w = torch.randn((num_row, num_col), dtype=torch.float32)\n",
    "b = torch.randn(num_row, dtype=torch.float32)\n",
    "x_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(x))\n",
    "w_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(w))\n",
    "b_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(b))\n",
    "\n",
    "ex = relax.build(ManualLinear, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "nd_res = vm[\"main\"](x_tvm,w_tvm,b_tvm)\n",
    "r0 = torch.mm(x, w.T)\n",
    "r1 = torch.add(r0, b)\n",
    "r2 = torch.nn.functional.gelu(r1)\n",
    "print(r2)\n",
    "print(nd_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw-tvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
