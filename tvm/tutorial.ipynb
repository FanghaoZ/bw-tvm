{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutotial is about one of the case we have done to run an e2e test with the TVM framework using our Brainwave FPGA hardware.\n",
    "\n",
    "It more like a POC to verify that we can use TVM to help us improve our BrianWave development. So please feel free to add more funtions as your like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running it, you need to create a conda env and install required packedge:\n",
    "\n",
    "    conda create -n bw-tvm python=3.8\n",
    "    conda activate bw-tvm\n",
    "    pip install mlc-ai-nightly -f https://mlc.ai/wheels\n",
    "    pip intall -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code we need to run any firmware test in BrainSlice repo to build the target files.\n",
    "Here we need to add PYTHONPATH to where the BrainSlice repo is.  \n",
    "1.Set the BRAINSLICE_PATH to the BrainSlice repo  \n",
    "2.Set SKU_PATH to the SKU fils you want to test.  \n",
    "3.FIRMWARE_PATH to the firmware you want to call from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "BRAINSLICE_PATH = 'D:/BrainSlice-repo/develop/BrainSlice/'\n",
    "BRAINSLICE_TARGET_PATH = BRAINSLICE_PATH + 'target/distrib/retail/x64/app/BrainSlice/'\n",
    "SKU_PATH = BRAINSLICE_PATH + 'src/config/skugen/obj/amd64/BERT-NP/SKU.json'\n",
    "FIRMWARE_LIB_PATH = BRAINSLICE_TARGET_PATH + '/Firmware/content'\n",
    "EMULATOR_PATH = BRAINSLICE_TARGET_PATH + '/DevKit/lib/native/python'\n",
    "FIRMWARE_PATH = FIRMWARE_LIB_PATH + '/BERT'\n",
    "sys.path.append(EMULATOR_PATH)\n",
    "sys.path.append(FIRMWARE_LIB_PATH)\n",
    "import brainslice_client as bs_client\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T\n",
    "import torch\n",
    "import numpy as np\n",
    "from ISA import Mem\n",
    "import Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Create IR model from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regisiter externl funtions to call FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fpga_session(mode='fp32', sku_dir=\" \", firmware_dir=\" \", firmware_name=\" \", fpga_chip_id=0, ):\n",
    "    _mode = {'fp32': bs_client.Backend.EMULATOR_FLOAT,\n",
    "             'quantized': bs_client.Backend.EMULATOR_QUANTIZED,\n",
    "             'FPGA': bs_client.Backend.FPGA}[mode]\n",
    "    directory_path = Path(firmware_dir)\n",
    "    bs_sess = bs_client.Session(_mode, Path(sku_dir), giano_instance_id=0, fpga_chip_id=fpga_chip_id, fpga_request_timeout=600.0)\n",
    "    bs_sess.start_session()\n",
    "    if mode == 'FPGA':\n",
    "        print(\"FPGA Session started,id: \", fpga_chip_id)\n",
    "        bs_sess.load_firmware(firmware_directory=directory_path,\n",
    "                              firmware_name=firmware_name)\n",
    "    else :\n",
    "        print(\"Emulator Session started,mode:\", mode)\n",
    "        bs_sess.load_emulator_firmware(firmware_directory=directory_path,\n",
    "                                           firmware_name=firmware_name)\n",
    "    global _native_dim\n",
    "    _native_dim = bs_sess.parameters.NATIVE_DIM\n",
    "    print(\"Firmware loaded successfully\")\n",
    "    return bs_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.register_func(\"bs.linear\",override=True)\n",
    "def bs_linear(x: tvm.nd.NDArray,\n",
    "              w: tvm.nd.NDArray,\n",
    "              b: tvm.nd.NDArray,\n",
    "              out: tvm.nd.NDArray):\n",
    "    ############################\n",
    "    #create fpga session\n",
    "    ############################\n",
    "    bs_sess = create_fpga_session(mode='fp32', sku_dir=SKU_PATH, firmware_dir=FIRMWARE_PATH, firmware_name=\"BERT\")\n",
    "\n",
    "    ############################\n",
    "    #init linear\n",
    "    ############################\n",
    "    # checke size\n",
    "    num_vec = x.shape[0]\n",
    "    num_col = x.shape[1]\n",
    "    num_row = w.shape[0]\n",
    "\n",
    "    assert w.shape[1] == num_col\n",
    "    assert b.shape[0] == num_row\n",
    "\n",
    "    x = x.numpy()\n",
    "    w = w.numpy()\n",
    "    b = b.numpy()\n",
    "    z = torch.zeros(num_vec, num_row, dtype=torch.float32)\n",
    "    out_torch = torch.from_dlpack(out)\n",
    "\n",
    "    # set the last element in MVM_IVRF to zero-vector\n",
    "    bs_sess.load_vector(np.zeros(bs_sess.parameters.NATIVE_DIM), Mem.MvmInitialVrf, bs_sess.parameters.MVM_INITIAL_VRF_SIZE - 1)\n",
    "\n",
    "    # SLU-specific matrix loading\n",
    "    if (bs_sess.parameters.ENABLE_FIXED_FUNCTION_SLU):\n",
    "        i_mat = np.identity(bs_sess.parameters.NATIVE_DIM)\n",
    "        bs_sess.load_matrix(i_mat, address = 0, memory = Mem.MatrixRf)\n",
    "\n",
    "    args = Bert.FullyConnectedParams(\n",
    "            rows = num_row,\n",
    "            cols = num_col,\n",
    "            bias = True,\n",
    "            gelu = True,\n",
    "            relu = False,\n",
    "            vecs = num_vec,\n",
    "            x_startaddr = 0,\n",
    "            w_startaddr = 1,\n",
    "            b_startaddr = 0,\n",
    "            imat_addr = 0,\n",
    "            asvrf1_scratchpad = 0,\n",
    "            mfu_scratchpad = 0,\n",
    "            use_dram = True,\n",
    "            weight_dram_addr = 0,\n",
    "            bias_dram_addr = 0,\n",
    "    )\n",
    "    # load bias and weight\n",
    "    bs_sess.load_vector(b, address = args.bias_dram_addr, memory = Mem.Dram)\n",
    "    bs_sess.load_matrix(w, address = args.weight_dram_addr, memory = Mem.Dram)\n",
    "    ############################\n",
    "    #run linear\n",
    "    ############################\n",
    "    bs_res = bs_sess.run(Bert.FullyConnectedLayer(args, inputVector = x))  # execute matrix to vector multiplications\n",
    "    torch.add(torch.asarray(bs_res[\"outputVector\"]), z, out=out_torch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Build relax IR module to call our FPGA.\n",
    "Ideally, this module could be automatically generated. With patten matching and translate, we can turn\n",
    "\n",
    "    @tvm.script.ir_module\n",
    "        class LinearModule:\n",
    "            @R.function\n",
    "            def main(x: R.Tensor((\"v\", \"c\"), \"float32\"),\n",
    "                     w: R.Tensor((\"c\", \"r\"), \"float32\"),\n",
    "                     b: R.Tensor((\"r\", ), \"float32\")\n",
    "                     ) -> R.Tensor((\"v\", \"r\"), \"float32\"):\n",
    "                v, c, r= T.int64(), T.int64(), T.int64()\n",
    "                with R.dataflow():\n",
    "                    z1 = R.matmul(x, w)\n",
    "                    z2 = R.add(z1, b)\n",
    "                    z3 = R.nn.gelu(z2)\n",
    "                    R.output(z3)\n",
    "                return z3\n",
    "\n",
    "to\n",
    "\n",
    "    @tvm.script.ir_module\n",
    "        class LinearModule:\n",
    "            @R.function\n",
    "            def main(x: R.Tensor((\"v\", \"c\"), \"float32\"),\n",
    "                    w: R.Tensor((\"r\", \"c\"), \"float32\"),\n",
    "                    b: R.Tensor((\"r\", ), \"float32\")\n",
    "                    ) -> R.Tensor((\"v\", \"r\"), \"float32\"):\n",
    "                v, c, r= T.int64(), T.int64(), T.int64()\n",
    "                with R.dataflow():\n",
    "                    z = R.call_dps_packed(\"bs.linear\", (x, w, b), R.Tensor((v, r), \"float32\"))\n",
    "                    R.output(lv0)\n",
    "                return z\n",
    "\n",
    "However, we have not solved this problem yet. So here we use our handwritten relax IR module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class ManualLinear:\n",
    "    @R.function\n",
    "    def main(x: R.Tensor((\"v\", \"c\"), \"float32\"),\n",
    "             w: R.Tensor((\"r\", \"c\"), \"float32\"),\n",
    "             b: R.Tensor((\"r\", ), \"float32\")\n",
    "             ) -> R.Tensor((\"v\", \"r\"), \"float32\"):\n",
    "        v, c, r= T.int64(), T.int64(), T.int64()\n",
    "        with R.dataflow():\n",
    "            lv0 = R.call_dps_packed(\"bs.linear\", (x, w, b), R.Tensor((v, r), \"float32\"))\n",
    "            R.output(lv0)\n",
    "        return lv0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Build and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emulator Session started,mode: fp32\n",
      "Firmware loaded successfully\n",
      "tensor([[-1.6996e-01, -1.5929e-05,  8.9435e-01,  1.0604e+00, -3.5851e-03,\n",
      "          0.0000e+00,  9.3218e+00,  0.0000e+00,  4.9787e-01,  3.6981e+00,\n",
      "         -2.2366e-04,  0.0000e+00, -5.4929e-03,  8.8425e+00,  5.9610e+00,\n",
      "         -1.6971e-01]])\n",
      "[[-1.7004395e-01 -5.3644180e-06  8.9746094e-01  1.0615234e+00\n",
      "  -3.1871796e-03  0.0000000e+00  9.3281250e+00  0.0000000e+00\n",
      "   4.9902344e-01  3.6953125e+00 -1.3720989e-04  0.0000000e+00\n",
      "  -5.0773621e-03  8.8437500e+00  5.9609375e+00 -1.6979980e-01]]\n"
     ]
    }
   ],
   "source": [
    "num_vec = 1\n",
    "num_col = 32\n",
    "num_row = 16\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "x = torch.randn((num_vec, num_col), dtype=torch.float32)\n",
    "w = torch.randn((num_row, num_col), dtype=torch.float32)\n",
    "b = torch.randn(num_row, dtype=torch.float32)\n",
    "x_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(x))\n",
    "w_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(w))\n",
    "b_tvm = tvm.nd.from_dlpack(torch.utils.dlpack.to_dlpack(b))\n",
    "\n",
    "ex = relax.build(ManualLinear, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "nd_res = vm[\"main\"](x_tvm,w_tvm,b_tvm)\n",
    "r0 = torch.mm(x, w.T)\n",
    "r1 = torch.add(r0, b)\n",
    "r2 = torch.nn.functional.gelu(r1)\n",
    "print(r2)\n",
    "print(nd_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw-tvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
